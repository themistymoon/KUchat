{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KUchat Production Deployment\n",
        "\n",
        "**Enterprise-Grade AI Assistant for Kasetsart University Curriculum Information**\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a complete production deployment of the KUchat AI assistant system, featuring:\n",
        "\n",
        "- GPT-OSS-120B language model (120 billion parameters)\n",
        "- 4-bit quantization using Unsloth optimization framework\n",
        "- Retrieval-Augmented Generation (RAG) system with ChromaDB\n",
        "- Web search integration (DuckDuckGo and Wikipedia)\n",
        "- Public-facing Gradio web interface\n",
        "- Zero local configuration required\n",
        "\n",
        "## System Requirements\n",
        "\n",
        "1. **Google Colab Pro+ subscription** (A100 GPU access required)\n",
        "2. **Documentation repository** (automatically downloaded from GitHub)\n",
        "3. **Internet connection** for model downloads and web search\n",
        "\n",
        "## Deployment Instructions\n",
        "\n",
        "1. Navigate to Runtime → Change runtime type → Select **A100 GPU**\n",
        "2. Execute all cells sequentially from top to bottom\n",
        "3. Wait for automatic documentation download from repository\n",
        "4. Access the generated public URL for the demo interface\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "- **Model**: GPT-OSS-120B (120B parameters)\n",
        "- **Quantization**: 4-bit BitsAndBytes (Unsloth optimized)\n",
        "- **Memory Footprint**: Approximately 40GB VRAM\n",
        "- **Inference Speed**: 30-60 tokens per second\n",
        "- **Optimization**: 2x faster inference, 75% memory reduction vs FP16\n",
        "- **Quality**: Production-ready with enterprise-grade responses\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install torch transformers accelerate bitsandbytes\n",
        "!pip install langchain langchain-community chromadb sentence-transformers\n",
        "!pip install duckduckgo-search wikipedia beautifulsoup4\n",
        "!pip install pypdf python-multipart aiofiles\n",
        "!pip install gradio\n",
        "\n",
        "print(\"All packages installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Libraries and Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import Optional, List, Tuple\n",
        "import gradio as gr\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from duckduckgo_search import DDGS\n",
        "import wikipedia\n",
        "import time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"KUCHAT PRODUCTION SYSTEM - A100 GPU\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    if 'A100' not in torch.cuda.get_device_name(0):\n",
        "        print(\"\\nWARNING: A100 GPU not detected\")\n",
        "        print(\"Action required: Runtime → Change runtime type → A100 GPU\")\n",
        "else:\n",
        "    print(\"\\nERROR: No GPU detected\")\n",
        "    print(\"Please enable GPU in runtime settings\")\n",
        "\n",
        "print(\"\\nLibraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: System Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration - Unsloth 4-bit Quantized GPT-OSS-120B\n",
        "MODEL_NAME = \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Documentation folder path\n",
        "DOCS_FOLDER = \"./docs\"\n",
        "\n",
        "# HuggingFace authentication token (optional for this model)\n",
        "HF_TOKEN = \"\"  # Add token from https://huggingface.co/settings/tokens if needed\n",
        "\n",
        "print(\"Configuration initialized\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Quantization: 4-bit BitsAndBytes (Unsloth optimized)\")\n",
        "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
        "print(f\"Documentation Path: {DOCS_FOLDER}\")\n",
        "print(f\"Expected VRAM Usage: ~40GB (A100 compatible)\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    print(\"HuggingFace token: Configured\")\n",
        "else:\n",
        "    print(\"HuggingFace token: Not required for this model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Download Documentation from Repository\n",
        "\n",
        "Documentation files are automatically downloaded from the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Downloading documentation from GitHub repository...\\n\")\n",
        "\n",
        "# Clone repository with sparse checkout (docs folder only)\n",
        "!git clone --depth 1 --filter=blob:none --sparse https://github.com/themistymoon/KUchat.git\n",
        "%cd KUchat\n",
        "!git sparse-checkout set docs\n",
        "\n",
        "# Move docs folder from /content/KUchat/docs to /content/docs\n",
        "%cd /content\n",
        "!mv KUchat/docs docs\n",
        "!rm -rf KUchat\n",
        "\n",
        "print(\"\\nDocumentation downloaded successfully\")\n",
        "print(f\"Location: {DOCS_FOLDER}\")\n",
        "\n",
        "# Display folder structure\n",
        "!echo \"\\nFolder structure:\"\n",
        "!ls -lh docs/ | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load GPT-OSS-120B Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"[1/4] Loading GPT-OSS-120B Language Model\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Estimated loading time: 3-5 minutes (first run)\")\n",
        "print(\"\\nModel: GPT-OSS-120B (120 billion parameters)\")\n",
        "print(\"Quantization: 4-bit BitsAndBytes (Unsloth optimized)\")\n",
        "print(\"Expected VRAM usage: ~40GB\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure model parameters\n",
        "max_seq_length = 2048  # Maximum context length (supports up to 32K)\n",
        "\n",
        "# Load model using Unsloth's FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,  # Automatic dtype detection\n",
        "    load_in_4bit=True,  # Enable 4-bit quantization\n",
        ")\n",
        "\n",
        "# Optimize for inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nModel loaded successfully in {load_time:.1f} seconds\")\n",
        "print(f\"Model: GPT-OSS-120B (120B parameters)\")\n",
        "print(f\"Quantization: 4-bit BitsAndBytes\")\n",
        "print(f\"Maximum sequence length: {max_seq_length} tokens\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "    print(f\"Memory optimization: ~75% reduction vs FP16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[2/4] Initializing Retrieval-Augmented Generation system\\n\")\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.docs_loaded = False\n",
        "        print(\"Embedding model initialized\")\n",
        "\n",
        "    def load_documents(self, folder_path: str):\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"WARNING: Folder {folder_path} not found\")\n",
        "            return\n",
        "\n",
        "        print(f\"Loading documents from {folder_path}...\\n\")\n",
        "        documents = []\n",
        "        file_count = 0\n",
        "        \n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.pdf'):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        loader = PyPDFLoader(file_path)\n",
        "                        documents.extend(loader.load())\n",
        "                        file_count += 1\n",
        "                        if file_count % 10 == 0:\n",
        "                            print(f\"  Loaded {file_count} files...\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error loading {file}: {e}\")\n",
        "                elif file.endswith(('.txt', '.md')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        loader = TextLoader(file_path)\n",
        "                        documents.extend(loader.load())\n",
        "                        file_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error loading {file}: {e}\")\n",
        "\n",
        "        if documents:\n",
        "            print(f\"\\nSplitting {len(documents)} documents into chunks...\")\n",
        "            texts = self.text_splitter.split_documents(documents)\n",
        "            print(f\"Created {len(texts)} text chunks\")\n",
        "            \n",
        "            print(\"Building vector database...\")\n",
        "            self.vectorstore = Chroma.from_documents(texts, self.embeddings)\n",
        "            self.docs_loaded = True\n",
        "            \n",
        "            print(f\"\\nSuccessfully loaded {len(documents)} documents ({len(texts)} chunks)\")\n",
        "            print(\"RAG system operational\")\n",
        "        else:\n",
        "            print(\"WARNING: No documents found in folder\")\n",
        "\n",
        "    def query(self, question: str, k: int = 3):\n",
        "        if not self.docs_loaded or self.vectorstore is None:\n",
        "            return None\n",
        "        results = self.vectorstore.similarity_search(question, k=k)\n",
        "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "# Initialize RAG system\n",
        "rag_system = RAGSystem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<VSCode.Cell id=\"#VSC-8d7de5e1\" language=\"markdown\">\n",
        "## Step 7: Load Curriculum Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"[3/4] Loading documents into RAG system...\\n\")\n",
        "rag_system.load_documents(DOCS_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Initialize Web Search System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WebSearchSystem:\n",
        "    @staticmethod\n",
        "    def search_duckduckgo(query: str, max_results: int = 3):\n",
        "        try:\n",
        "            with DDGS() as ddgs:\n",
        "                results = list(ddgs.text(query, max_results=max_results))\n",
        "                return \"\\n\\n\".join([f\"**{r['title']}**\\n{r['body']}\" for r in results])\n",
        "        except Exception as e:\n",
        "            return f\"Web search failed: {str(e)}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def search_wikipedia(query: str):\n",
        "        try:\n",
        "            wikipedia.set_lang('th')\n",
        "            summary = wikipedia.summary(query, sentences=3)\n",
        "            return summary\n",
        "        except:\n",
        "            try:\n",
        "                wikipedia.set_lang('en')\n",
        "                summary = wikipedia.summary(query, sentences=3)\n",
        "                return summary\n",
        "            except Exception as e:\n",
        "                return f\"Wikipedia search failed: {str(e)}\"\n",
        "\n",
        "web_search = WebSearchSystem()\n",
        "print(\"Web search system initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Chat Function with RAG and Web Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_bot(\n",
        "    message: str,\n",
        "    history: List[Tuple[str, str]],\n",
        "    use_rag: bool,\n",
        "    use_web_search: bool,\n",
        "    temperature: float,\n",
        "    max_tokens: int\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Main chat function with RAG and web search\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Build context\n",
        "        context = \"\"\n",
        "        \n",
        "        if use_rag and rag_system.docs_loaded:\n",
        "            rag_context = rag_system.query(message, k=3)\n",
        "            if rag_context:\n",
        "                context += f\"\\n\\nเอกสารหลักสูตรที่เกี่ยวข้อง:\\n{rag_context}\"\n",
        "        \n",
        "        if use_web_search:\n",
        "            web_results = web_search.search_duckduckgo(message, max_results=2)\n",
        "            context += f\"\\n\\nข้อมูลจากอินเทอร์เน็ต:\\n{web_results}\"\n",
        "        \n",
        "        # Build conversation history\n",
        "        conversation = \"\"\n",
        "        for user_msg, bot_msg in history[-3:]:  # Last 3 exchanges\n",
        "            conversation += f\"คำถาม: {user_msg}\\nคำตอบ: {bot_msg}\\n\\n\"\n",
        "        \n",
        "        # Build prompt\n",
        "        prompt = f\"\"\"คุณคือผู้ช่วยตอบคำถามเกี่ยวกับหลักสูตรมหาวิทยาลัยเกษตรศาสตร์ (Kasetsart University)\n",
        "ให้ตอบคำถามอย่างละเอียด เป็นมิตร และถูกต้อง\n",
        "\n",
        "{conversation}\n",
        "คำถามใหม่: {message}\n",
        "{context}\n",
        "\n",
        "คำตอบ:\"\"\"\n",
        "        \n",
        "        # Generate response\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                top_k=50,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract answer (remove prompt)\n",
        "        if \"คำตอบ:\" in response:\n",
        "            answer = response.split(\"คำตอบ:\")[-1].strip()\n",
        "        else:\n",
        "            answer = response[len(prompt):].strip()\n",
        "        \n",
        "        return answer\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"Chat function initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Launch Gradio Demo Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[4/4] Launching Gradio interface...\\n\")\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(\n",
        "    title=\"KUchat - Kasetsart University AI Assistant\",\n",
        "    theme=gr.themes.Soft()\n",
        ") as demo:\n",
        "    \n",
        "    gr.Markdown(\"\"\"\n",
        "    # KUchat - Kasetsart University AI Assistant\n",
        "    \n",
        "    ผู้ช่วยตอบคำถามเกี่ยวกับหลักสูตรมหาวิทยาลัยเกษตรศาสตร์\n",
        "    \n",
        "    **Powered by GPT-OSS-120B (4-bit, Unsloth) on A100 GPU**\n",
        "    \n",
        "    ---\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=500,\n",
        "                label=\"Chat History\",\n",
        "                show_copy_button=True\n",
        "            )\n",
        "            \n",
        "            msg = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"ถามคำถามเกี่ยวกับหลักสูตร เช่น 'หลักสูตรวิศวกรรมคอมพิวเตอร์มีอะไรบ้าง'\",\n",
        "                lines=2\n",
        "            )\n",
        "            \n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"Clear\")\n",
        "        \n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Settings\")\n",
        "            \n",
        "            use_rag = gr.Checkbox(\n",
        "                label=\"Use RAG (Curriculum Documents)\",\n",
        "                value=True,\n",
        "                info=\"Search in curriculum documents\"\n",
        "            )\n",
        "            \n",
        "            use_web_search = gr.Checkbox(\n",
        "                label=\"Use Web Search\",\n",
        "                value=False,\n",
        "                info=\"Search online for latest information\"\n",
        "            )\n",
        "            \n",
        "            temperature = gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=1.0,\n",
        "                value=0.7,\n",
        "                step=0.1,\n",
        "                label=\"Temperature\",\n",
        "                info=\"Higher values increase creativity\"\n",
        "            )\n",
        "            \n",
        "            max_tokens = gr.Slider(\n",
        "                minimum=128,\n",
        "                maximum=2048,\n",
        "                value=512,\n",
        "                step=128,\n",
        "                label=\"Max Tokens\",\n",
        "                info=\"Maximum response length\"\n",
        "            )\n",
        "            \n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            ### System Information\n",
        "            - **Model**: GPT-OSS-120B\n",
        "            - **Parameters**: 120B (4-bit)\n",
        "            - **GPU**: A100 80GB\n",
        "            - **VRAM Usage**: ~40GB\n",
        "            - **Inference Speed**: 30-60 tokens/sec\n",
        "            - **Optimization**: Unsloth Framework\n",
        "            - **RAG Database**: ChromaDB\n",
        "            - **Documents**: 170+ curricula\n",
        "            \"\"\")\n",
        "    \n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### Example Questions:\n",
        "    - หลักสูตรวิศวกรรมคอมพิวเตอร์มีอะไรบ้าง\n",
        "    - คณะวิทยาศาสตร์มีกี่สาขา\n",
        "    - ค่าเทอมคณะบริหารธุรกิจเท่าไหร่\n",
        "    - วิทยาการคอมพิวเตอร์ต่างจากวิศวกรรมคอมพิวเตอร์อย่างไร\n",
        "    \"\"\")\n",
        "    \n",
        "    # Chat function\n",
        "    def respond(message, chat_history, use_rag, use_web_search, temperature, max_tokens):\n",
        "        bot_message = chat_with_bot(\n",
        "            message, chat_history, use_rag, use_web_search, temperature, max_tokens\n",
        "        )\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "    \n",
        "    # Event handlers\n",
        "    submit_btn.click(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, use_rag, use_web_search, temperature, max_tokens],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "    \n",
        "    msg.submit(\n",
        "        respond,\n",
        "        inputs=[msg, chatbot, use_rag, use_web_search, temperature, max_tokens],\n",
        "        outputs=[msg, chatbot]\n",
        "    )\n",
        "    \n",
        "    clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Launch with public URL\n",
        "print(\"=\"*60)\n",
        "print(\"LAUNCHING KUCHAT DEMO\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nStarting Gradio interface...\\n\")\n",
        "\n",
        "demo.launch(\n",
        "    share=True,  # Creates public URL\n",
        "    debug=False,\n",
        "    show_error=True,\n",
        "    server_port=7860\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KUCHAT IS LIVE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nPublic URL generated above\")\n",
        "print(\"Share the URL with anyone to use the chatbot\")\n",
        "print(\"Keep this cell running to keep the demo alive\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Demo Features\n",
        "\n",
        "### Chat Interface\n",
        "- Gradio UI with chat history\n",
        "- Real-time responses from GPT-OSS-120B\n",
        "- Copy/paste support\n",
        "\n",
        "### Controls\n",
        "- **RAG Toggle**: Search curriculum documents\n",
        "- **Web Search**: Get latest online information\n",
        "- **Temperature**: Adjust creativity (0.1-1.0)\n",
        "- **Max Tokens**: Control response length\n",
        "\n",
        "### Public Access\n",
        "- Share URL works for 72 hours\n",
        "- Anyone can access without login\n",
        "- Multiple users can chat simultaneously\n",
        "\n",
        "### Performance\n",
        "- **Model**: GPT-OSS-120B (120B parameters)\n",
        "- **Quantization**: 4-bit BNB (Unsloth optimized)\n",
        "- **GPU**: A100 80GB (~40GB VRAM used)\n",
        "- **Speed**: 30-60 tokens/second (2x faster)\n",
        "- **Quality**: Production-ready\n",
        "- **VRAM Savings**: 75% compared to FP16\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### No A100 GPU?\n",
        "- Go to: Runtime → Change runtime type → A100 GPU\n",
        "- Requires Google Colab Pro+ subscription\n",
        "\n",
        "### Model loading fails?\n",
        "- Check HuggingFace token is set\n",
        "- Verify internet connection\n",
        "- Try restarting runtime\n",
        "\n",
        "### Demo stops working?\n",
        "- Cell must keep running for demo to work\n",
        "- Colab disconnects after ~12 hours idle\n",
        "- Re-run cell 10 to restart demo\n",
        "\n",
        "---\n",
        "\n",
        "## Cost Estimate\n",
        "\n",
        "**Google Colab Pro+**: ~$50/month\n",
        "- A100 GPU access\n",
        "- ~$1-2 per hour of usage\n",
        "- Background execution\n",
        "- Priority access\n",
        "\n",
        "**Alternative**: Use test version (T4 GPU) for free\n",
        "- See `colab_backend_test.ipynb`\n",
        "- Smaller model but still functional\n",
        "- Good for testing/development\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
