{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KUchat Test Backend - Google Colab Free/T4 GPU\n",
        "\n",
        "**Lightweight version for testing on Google Colab Free with T4 GPU**\n",
        "\n",
        "This notebook uses smaller models suitable for free Colab resources:\n",
        "- **Text Model**: Qwen/Qwen2-7B-Instruct (7B parameters - fits on T4)\n",
        "- **GPU**: T4 (15GB VRAM) - Available on Colab Free\n",
        "- **Purpose**: Test functionality before deploying to A100\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Runtime**: Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
        "2. **Run cells** in order\n",
        "3. **Test** with the API endpoints\n",
        "4. If works well, upgrade to full version with A100 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required packages\n",
        "!pip install torch transformers accelerate bitsandbytes\n",
        "!pip install langchain langchain-community chromadb sentence-transformers\n",
        "!pip install fastapi uvicorn pydantic\n",
        "!pip install duckduckgo-search wikipedia beautifulsoup4\n",
        "!pip install pypdf python-multipart aiofiles\n",
        "!pip install gradio\n",
        "!pip install pyngrok\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import StreamingResponse\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, List\n",
        "import uvicorn\n",
        "import json\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import chromadb\n",
        "from duckduckgo_search import DDGS\n",
        "import wikipedia\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíé GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration (Optimized for T4 GPU)\n",
        "MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"  # 7B model fits on T4\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Use 4-bit quantization to save memory\n",
        "USE_4BIT = True\n",
        "\n",
        "# Docs folder path\n",
        "DOCS_FOLDER = \"./docs\"\n",
        "\n",
        "# Server config\n",
        "PORT = 8000\n",
        "\n",
        "print(\"‚úÖ Configuration set!\")\n",
        "print(f\"üì¶ Model: {MODEL_NAME}\")\n",
        "print(f\"üî¢ 4-bit Quantization: {USE_4BIT}\")\n",
        "print(f\"üìÅ Docs folder: {DOCS_FOLDER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Download Documents from GitHub\n",
        "\n",
        "**Option 1: Download docs folder from GitHub (Recommended)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download only docs folder using sparse checkout\n",
        "!git clone --depth 1 --filter=blob:none --sparse https://github.com/themistymoon/KUchat.git\n",
        "%cd KUchat\n",
        "!git sparse-checkout set docs\n",
        "\n",
        "# Move docs to current directory\n",
        "!mv docs /content/docs\n",
        "%cd /content\n",
        "!rm -rf KUchat\n",
        "\n",
        "print(\"‚úÖ Documents downloaded from GitHub!\")\n",
        "print(f\"üìÅ Documents location: {DOCS_FOLDER}\")\n",
        "\n",
        "# List downloaded files\n",
        "!ls -lh docs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Option 2: Upload from Google Drive (Alternative)**\n",
        "\n",
        "Uncomment and run this cell if you want to use files from Google Drive instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r /content/drive/MyDrive/KUchat/docs ./docs\n",
        "# print(\"‚úÖ Documents copied from Google Drive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Load AI Model (7B - T4 Compatible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"KUCHAT TEST BACKEND - T4 GPU VERSION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n[1/4] Loading AI model: {MODEL_NAME}...\")\n",
        "print(\"‚è≥ This will take 2-5 minutes on first run...\\n\")\n",
        "\n",
        "# Configure 4-bit quantization for T4 GPU\n",
        "if USE_4BIT:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded with 4-bit quantization (saves 75% memory!)\")\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ Model loaded in FP16\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üíæ Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B parameters\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üî• GPU memory used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Initialize RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[2/4] Initializing RAG system...\")\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.docs_loaded = False\n",
        "\n",
        "    def load_documents(self, folder_path: str):\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"‚ö†Ô∏è  Folder {folder_path} not found\")\n",
        "            return\n",
        "\n",
        "        documents = []\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.pdf'):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        loader = PyPDFLoader(file_path)\n",
        "                        documents.extend(loader.load())\n",
        "                        print(f\"  üìÑ Loaded: {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ‚ùå Error loading {file}: {e}\")\n",
        "                elif file.endswith(('.txt', '.md')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        loader = TextLoader(file_path)\n",
        "                        documents.extend(loader.load())\n",
        "                        print(f\"  üìÑ Loaded: {file}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ‚ùå Error loading {file}: {e}\")\n",
        "\n",
        "        if documents:\n",
        "            texts = self.text_splitter.split_documents(documents)\n",
        "            self.vectorstore = Chroma.from_documents(texts, self.embeddings)\n",
        "            self.docs_loaded = True\n",
        "            print(f\"‚úÖ Successfully loaded {len(documents)} documents ({len(texts)} chunks)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No documents found in folder\")\n",
        "\n",
        "    def query(self, question: str, k: int = 3):\n",
        "        if not self.docs_loaded or self.vectorstore is None:\n",
        "            return None\n",
        "        results = self.vectorstore.similarity_search(question, k=k)\n",
        "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "# Initialize RAG\n",
        "rag_system = RAGSystem()\n",
        "print(\"‚úÖ RAG system initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Load Documents into RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[3/4] Loading documents from ./docs folder...\\n\")\n",
        "rag_system.load_documents(DOCS_FOLDER)\n",
        "print(\"\\n‚úÖ Documents loaded into RAG system!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Web Search System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WebSearchSystem:\n",
        "    @staticmethod\n",
        "    def search_duckduckgo(query: str, max_results: int = 3):\n",
        "        try:\n",
        "            with DDGS() as ddgs:\n",
        "                results = list(ddgs.text(query, max_results=max_results))\n",
        "                return \"\\n\\n\".join([f\"**{r['title']}**\\n{r['body']}\" for r in results])\n",
        "        except Exception as e:\n",
        "            return f\"Web search failed: {str(e)}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def search_wikipedia(query: str):\n",
        "        try:\n",
        "            wikipedia.set_lang('th')\n",
        "            summary = wikipedia.summary(query, sentences=3)\n",
        "            return summary\n",
        "        except:\n",
        "            try:\n",
        "                wikipedia.set_lang('en')\n",
        "                summary = wikipedia.summary(query, sentences=3)\n",
        "                return summary\n",
        "            except Exception as e:\n",
        "                return f\"Wikipedia search failed: {str(e)}\"\n",
        "\n",
        "web_search = WebSearchSystem()\n",
        "print(\"‚úÖ Web search system initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ FastAPI Backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app = FastAPI(title=\"KUchat Test Backend - T4 GPU\")\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    question: str\n",
        "    max_tokens: int = 512\n",
        "    temperature: float = 0.7\n",
        "    use_rag: bool = True\n",
        "    use_web_search: bool = False\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"status\": \"running\",\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"version\": \"test-t4\",\n",
        "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": True,\n",
        "        \"rag_loaded\": rag_system.docs_loaded,\n",
        "        \"gpu_available\": torch.cuda.is_available()\n",
        "    }\n",
        "\n",
        "@app.post(\"/query\")\n",
        "async def query(request: QueryRequest):\n",
        "    try:\n",
        "        # Build context\n",
        "        context = \"\"\n",
        "        \n",
        "        if request.use_rag and rag_system.docs_loaded:\n",
        "            rag_context = rag_system.query(request.question)\n",
        "            if rag_context:\n",
        "                context += f\"\\n\\n‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\\n{rag_context}\"\n",
        "        \n",
        "        if request.use_web_search:\n",
        "            web_results = web_search.search_duckduckgo(request.question)\n",
        "            context += f\"\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ß‡πá‡∏ö:\\n{web_results}\"\n",
        "        \n",
        "        # Build prompt\n",
        "        prompt = f\"\"\"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {request.question}\n",
        "{context}\n",
        "\n",
        "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\"\"\"\n",
        "        \n",
        "        # Generate response\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=request.max_tokens,\n",
        "            temperature=request.temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract answer (remove prompt)\n",
        "        answer = response.split(\"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\")[-1].strip()\n",
        "        \n",
        "        return {\n",
        "            \"response\": answer,\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"used_rag\": request.use_rag and rag_system.docs_loaded,\n",
        "            \"used_web_search\": request.use_web_search\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"‚úÖ FastAPI backend created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîü Setup Ngrok Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Ngrok auth token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = \"\"  # Paste your token here\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"‚úÖ Ngrok authenticated!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Please set NGROK_AUTH_TOKEN above\")\n",
        "    print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Start Backend Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[4/4] Starting backend server...\\n\")\n",
        "\n",
        "# Start Uvicorn in background thread\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
        "\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Start Ngrok tunnel\n",
        "public_url = ngrok.connect(PORT)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ BACKEND SERVER IS RUNNING!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüåê Public URL: {public_url}\")\n",
        "print(f\"üìù API Documentation: {public_url}/docs\")\n",
        "print(f\"\\nüí° Use this URL in your frontend_app.py:\")\n",
        "print(f\"   API_URL = \\\"{public_url}\\\"\")\n",
        "print(\"\\n‚è∏Ô∏è  Keep this cell running to keep the server alive!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Test the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Test health endpoint\n",
        "response = requests.get(f\"{public_url}/health\")\n",
        "print(\"Health Check:\")\n",
        "print(response.json())\n",
        "\n",
        "# Test query\n",
        "test_query = {\n",
        "    \"question\": \"‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á\",\n",
        "    \"max_tokens\": 256,\n",
        "    \"temperature\": 0.7,\n",
        "    \"use_rag\": True,\n",
        "    \"use_web_search\": False\n",
        "}\n",
        "\n",
        "print(\"\\nTest Query:\")\n",
        "response = requests.post(f\"{public_url}/query\", json=test_query)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Next Steps\n",
        "\n",
        "1. **Copy the public URL** from above\n",
        "2. **Update frontend_app.py** with the URL\n",
        "3. **Run frontend** on your local computer\n",
        "4. **Test the chatbot**!\n",
        "\n",
        "If everything works well, you can upgrade to the full version with:\n",
        "- **GPU**: A100 (80GB)\n",
        "- **Model**: Qwen3-Omni-30B + GPT-OSS-120B\n",
        "- **Performance**: 20-50 tokens/second\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Performance Comparison\n",
        "\n",
        "| Feature | Test (T4) | Production (A100) |\n",
        "|---------|-----------|-------------------|\n",
        "| GPU | T4 (15GB) | A100 (80GB) |\n",
        "| Model | Qwen2-7B | Qwen3-Omni-30B + GPT-OSS-120B |\n",
        "| Speed | 5-10 tok/s | 20-50 tok/s |\n",
        "| Quality | Good | Excellent |\n",
        "| Cost | Free | ~$1/hour |\n",
        "| Purpose | Testing | Production |\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}