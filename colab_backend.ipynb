{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecb5957",
   "metadata": {},
   "source": [
    "# Multi-Model AI Backend with FastAPI and RAG\n",
    "## Models: Qwen3-Omni-30B-A3B-Instruct & GPT-OSS-120B\n",
    "### Hardware: A100 80GB GPU (Google Colab Pro Plus)\n",
    "\n",
    "**Important Setup Steps:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí A100 GPU\n",
    "2. Ensure you have Colab Pro Plus for A100 access\n",
    "3. Get your HuggingFace token for model access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26423a",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb20661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q fastapi uvicorn pyngrok python-multipart\n",
    "!pip install -q langchain chromadb sentence-transformers\n",
    "!pip install -q pypdf pillow pydub moviepy\n",
    "!pip install -q huggingface_hub librosa soundfile\n",
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d612b84",
   "metadata": {},
   "source": [
    "## Step 1.5: Install Web Search Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q duckduckgo-search beautifulsoup4 requests-html\n",
    "!pip install -q googlesearch-python wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb50862",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! This notebook requires A100 GPU.\")\n",
    "\n",
    "# Login to HuggingFace (required for gated models)\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN\"  # Replace with your token\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff96e97",
   "metadata": {},
   "source": [
    "## Step 3: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading Qwen3-Omni-30B-A3B-Instruct (Multimodal)...\")\n",
    "qwen_model_name = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_model_name, trust_remote_code=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"‚úì Qwen3-Omni loaded successfully\")\n",
    "\n",
    "print(\"\\nLoading GPT-OSS-120B-Unsloth-BNB-4bit (Text Only)...\")\n",
    "gpt_model_name = \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\"\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_name, trust_remote_code=True)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gpt_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"‚úì GPT-OSS-120B loaded successfully\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "qwen_model.eval()\n",
    "gpt_model.eval()\n",
    "\n",
    "print(\"\\n=== Models Ready ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446583c8",
   "metadata": {},
   "source": [
    "## Step 4: Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab37bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import re\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing RAG system...\")\n",
    "        # Use a lightweight embedding model\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cuda'}\n",
    "        )\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.persist_directory = \"/content/chroma_db\"\n",
    "        self.vectorstore = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        # Store original documents for search\n",
    "        self.documents = []\n",
    "        self.document_metadata = []\n",
    "        print(\"‚úì RAG system initialized\")\n",
    "    \n",
    "    def add_documents(self, file_paths, file_types):\n",
    "        \"\"\"Add documents to the vector store\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for file_path, file_type in zip(file_paths, file_types):\n",
    "            try:\n",
    "                if file_type == \"pdf\":\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                elif file_type in [\"txt\", \"md\"]:\n",
    "                    loader = TextLoader(file_path)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Store metadata\n",
    "                for doc in docs:\n",
    "                    self.document_metadata.append({\n",
    "                        \"source\": file_path,\n",
    "                        \"filename\": Path(file_path).name,\n",
    "                        \"type\": file_type\n",
    "                    })\n",
    "                \n",
    "                documents.extend(docs)\n",
    "                self.documents.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        if documents:\n",
    "            # Split documents\n",
    "            splits = self.text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create or update vector store\n",
    "            if self.vectorstore is None:\n",
    "                self.vectorstore = Chroma.from_documents(\n",
    "                    documents=splits,\n",
    "                    embedding=self.embeddings,\n",
    "                    persist_directory=self.persist_directory\n",
    "                )\n",
    "            else:\n",
    "                self.vectorstore.add_documents(splits)\n",
    "            \n",
    "            return len(splits)\n",
    "        return 0\n",
    "    \n",
    "    def retrieve(self, query, k=4):\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            return []\n",
    "        \n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in docs]\n",
    "    \n",
    "    def search_documents(self, query, search_type=\"keyword\", k=10):\n",
    "        \"\"\"\n",
    "        Search through documents with multiple methods\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            search_type: \"keyword\", \"semantic\", or \"hybrid\"\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of search results with content and metadata\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            return {\"results\": [], \"total\": 0, \"message\": \"No documents uploaded\"}\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        if search_type == \"keyword\" or search_type == \"hybrid\":\n",
    "            # Keyword search (case-insensitive)\n",
    "            query_lower = query.lower()\n",
    "            for idx, doc in enumerate(self.documents):\n",
    "                content = doc.page_content\n",
    "                content_lower = content.lower()\n",
    "                \n",
    "                # Find all occurrences\n",
    "                if query_lower in content_lower:\n",
    "                    # Count occurrences\n",
    "                    count = content_lower.count(query_lower)\n",
    "                    \n",
    "                    # Extract context around matches\n",
    "                    matches = []\n",
    "                    start = 0\n",
    "                    while True:\n",
    "                        pos = content_lower.find(query_lower, start)\n",
    "                        if pos == -1:\n",
    "                            break\n",
    "                        \n",
    "                        # Get context (100 chars before and after)\n",
    "                        context_start = max(0, pos - 100)\n",
    "                        context_end = min(len(content), pos + len(query) + 100)\n",
    "                        context = content[context_start:context_end]\n",
    "                        \n",
    "                        # Add ellipsis if not at start/end\n",
    "                        if context_start > 0:\n",
    "                            context = \"...\" + context\n",
    "                        if context_end < len(content):\n",
    "                            context = context + \"...\"\n",
    "                        \n",
    "                        matches.append({\n",
    "                            \"position\": pos,\n",
    "                            \"context\": context\n",
    "                        })\n",
    "                        \n",
    "                        start = pos + 1\n",
    "                        if len(matches) >= 3:  # Limit to 3 matches per document\n",
    "                            break\n",
    "                    \n",
    "                    metadata = self.document_metadata[idx] if idx < len(self.document_metadata) else {}\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"document_index\": idx,\n",
    "                        \"filename\": metadata.get(\"filename\", \"Unknown\"),\n",
    "                        \"source\": metadata.get(\"source\", \"Unknown\"),\n",
    "                        \"match_count\": count,\n",
    "                        \"matches\": matches,\n",
    "                        \"relevance_score\": count  # Simple relevance by count\n",
    "                    })\n",
    "        \n",
    "        if search_type == \"semantic\" or search_type == \"hybrid\":\n",
    "            # Semantic search using vector similarity\n",
    "            if self.vectorstore is not None:\n",
    "                semantic_docs = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "                \n",
    "                for doc, score in semantic_docs:\n",
    "                    # Check if already in results (for hybrid)\n",
    "                    existing = None\n",
    "                    for r in results:\n",
    "                        if doc.page_content in [d.page_content for d in self.documents]:\n",
    "                            existing = r\n",
    "                            break\n",
    "                    \n",
    "                    if existing:\n",
    "                        # Boost relevance for hybrid results\n",
    "                        existing[\"relevance_score\"] += (1 - score) * 10\n",
    "                        existing[\"semantic_score\"] = float(1 - score)\n",
    "                    else:\n",
    "                        # Add as new result\n",
    "                        results.append({\n",
    "                            \"document_index\": -1,\n",
    "                            \"filename\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                            \"content\": doc.page_content[:500] + \"...\",\n",
    "                            \"semantic_score\": float(1 - score),\n",
    "                            \"relevance_score\": float(1 - score)\n",
    "                        })\n",
    "        \n",
    "        # Sort by relevance\n",
    "        results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "        \n",
    "        # Limit results\n",
    "        results = results[:k]\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"total\": len(results),\n",
    "            \"query\": query,\n",
    "            \"search_type\": search_type\n",
    "        }\n",
    "    \n",
    "    def list_documents(self):\n",
    "        \"\"\"List all uploaded documents\"\"\"\n",
    "        unique_docs = {}\n",
    "        for meta in self.document_metadata:\n",
    "            filename = meta.get(\"filename\", \"Unknown\")\n",
    "            if filename not in unique_docs:\n",
    "                unique_docs[filename] = {\n",
    "                    \"filename\": filename,\n",
    "                    \"source\": meta.get(\"source\", \"Unknown\"),\n",
    "                    \"type\": meta.get(\"type\", \"Unknown\")\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            \"documents\": list(unique_docs.values()),\n",
    "            \"total\": len(unique_docs)\n",
    "        }\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear the vector database\"\"\"\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "        self.document_metadata = []\n",
    "        import shutil\n",
    "        if Path(self.persist_directory).exists():\n",
    "            shutil.rmtree(self.persist_directory)\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def auto_load_documents_from_folder(rag_system, folder_path=\"/content/docs\"):\n",
    "    \"\"\"\n",
    "    Automatically load all PDF and text files from a folder and its subfolders\n",
    "    \n",
    "    Args:\n",
    "        rag_system: RAGSystem instance\n",
    "        folder_path: Path to the docs folder\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loading statistics\n",
    "    \"\"\"\n",
    "    print(f\"üîç Scanning for documents in: {folder_path}\")\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"‚ö†Ô∏è  Folder not found: {folder_path}\")\n",
    "        print(\"üí° You can upload documents manually via the API or create the docs folder\")\n",
    "        return {\"success\": False, \"message\": \"Folder not found\", \"files_loaded\": 0}\n",
    "    \n",
    "    # Find all supported files\n",
    "    supported_extensions = ['.pdf', '.txt', '.md']\n",
    "    file_paths = []\n",
    "    file_types = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_ext = Path(file).suffix.lower()\n",
    "            \n",
    "            if file_ext in supported_extensions:\n",
    "                file_paths.append(file_path)\n",
    "                # Map extension to type\n",
    "                if file_ext == '.pdf':\n",
    "                    file_types.append('pdf')\n",
    "                else:\n",
    "                    file_types.append('txt')\n",
    "                \n",
    "                print(f\"  üìÑ Found: {os.path.relpath(file_path, folder_path)}\")\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"‚ÑπÔ∏è  No documents found in the folder\")\n",
    "        return {\"success\": True, \"message\": \"No documents found\", \"files_loaded\": 0}\n",
    "    \n",
    "    # Load documents into RAG system\n",
    "    print(f\"\\nüìö Loading {len(file_paths)} documents into RAG system...\")\n",
    "    try:\n",
    "        chunks_added = rag_system.add_documents(file_paths, file_types)\n",
    "        print(f\"‚úÖ Successfully loaded {len(file_paths)} files ({chunks_added} chunks)\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"Documents loaded successfully\",\n",
    "            \"files_loaded\": len(file_paths),\n",
    "            \"chunks_created\": chunks_added,\n",
    "            \"files\": [os.path.relpath(fp, folder_path) for fp in file_paths]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading documents: {e}\")\n",
    "        return {\"success\": False, \"message\": str(e), \"files_loaded\": 0}\n",
    "\n",
    "# Auto-load documents from docs folder\n",
    "# Note: In Colab, you'll need to upload your docs folder or mount Google Drive\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUTO-LOADING DOCUMENTS FROM DOCS FOLDER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to auto-load from /content/docs\n",
    "auto_load_result = auto_load_documents_from_folder(rag_system, \"/content/docs\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä Auto-Load Summary:\")\n",
    "print(f\"  Status: {'‚úÖ Success' if auto_load_result['success'] else '‚ùå Failed'}\")\n",
    "print(f\"  Files Loaded: {auto_load_result.get('files_loaded', 0)}\")\n",
    "if 'chunks_created' in auto_load_result:\n",
    "    print(f\"  Chunks Created: {auto_load_result['chunks_created']}\")\n",
    "if auto_load_result.get('files'):\n",
    "    print(f\"\\n  Loaded Files:\")\n",
    "    for f in auto_load_result['files']:\n",
    "        print(f\"    - {f}\")\n",
    "\n",
    "print(\"\\nüí° TIP: You can still upload more documents via the /upload-docs API endpoint\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6bce6",
   "metadata": {},
   "source": [
    "## Step 4.1: Auto-Load Documents from Docs Folder\n",
    "\n",
    "This will automatically load all PDFs and text files from your `docs` folder and subfolders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741daddb",
   "metadata": {},
   "source": [
    "## Step 4.5: Setup Web Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import wikipedia\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "class WebSearchSystem:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Web Search system...\")\n",
    "        self.ddgs = DDGS()\n",
    "        wikipedia.set_lang(\"en\")  # Default to English, can be changed\n",
    "        print(\"‚úì Web Search system initialized\")\n",
    "    \n",
    "    def search_duckduckgo(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search using DuckDuckGo\"\"\"\n",
    "        try:\n",
    "            results = []\n",
    "            search_results = self.ddgs.text(query, max_results=max_results)\n",
    "            \n",
    "            for result in search_results:\n",
    "                results.append({\n",
    "                    'title': result.get('title', ''),\n",
    "                    'url': result.get('href', ''),\n",
    "                    'snippet': result.get('body', '')\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"DuckDuckGo search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_wikipedia(self, query: str, sentences: int = 3) -> Dict:\n",
    "        \"\"\"Search Wikipedia for a summary\"\"\"\n",
    "        try:\n",
    "            # Try English first\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            try:\n",
    "                summary = wikipedia.summary(query, sentences=sentences, auto_suggest=True)\n",
    "                page = wikipedia.page(query, auto_suggest=True)\n",
    "                return {\n",
    "                    'title': page.title,\n",
    "                    'summary': summary,\n",
    "                    'url': page.url,\n",
    "                    'source': 'Wikipedia (EN)'\n",
    "                }\n",
    "            except:\n",
    "                # Try Thai if English fails\n",
    "                wikipedia.set_lang(\"th\")\n",
    "                summary = wikipedia.summary(query, sentences=sentences, auto_suggest=True)\n",
    "                page = wikipedia.page(query, auto_suggest=True)\n",
    "                return {\n",
    "                    'title': page.title,\n",
    "                    'summary': summary,\n",
    "                    'url': page.url,\n",
    "                    'source': 'Wikipedia (TH)'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_webpage_content(self, url: str, max_chars: int = 2000) -> str:\n",
    "        \"\"\"Fetch and extract main content from a webpage\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up text\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Truncate to max_chars\n",
    "            return text[:max_chars]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def comprehensive_search(self, query: str, max_results: int = 5, \n",
    "                           include_wikipedia: bool = True,\n",
    "                           fetch_content: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform a comprehensive web search\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'web_results', 'wikipedia', and 'formatted_context'\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'web_results': [],\n",
    "            'wikipedia': None,\n",
    "            'formatted_context': ''\n",
    "        }\n",
    "        \n",
    "        # Search DuckDuckGo\n",
    "        print(f\"Searching web for: {query}\")\n",
    "        web_results = self.search_duckduckgo(query, max_results)\n",
    "        results['web_results'] = web_results\n",
    "        \n",
    "        # Search Wikipedia if enabled\n",
    "        if include_wikipedia:\n",
    "            wiki_result = self.search_wikipedia(query)\n",
    "            results['wikipedia'] = wiki_result\n",
    "        \n",
    "        # Optionally fetch full content from top results\n",
    "        if fetch_content and web_results:\n",
    "            for result in web_results[:2]:  # Only fetch top 2 to save time\n",
    "                content = self.fetch_webpage_content(result['url'])\n",
    "                if content:\n",
    "                    result['content'] = content\n",
    "        \n",
    "        # Format context for LLM\n",
    "        context_parts = []\n",
    "        \n",
    "        if results['wikipedia']:\n",
    "            context_parts.append(f\"Wikipedia Summary:\\\\n{results['wikipedia']['summary']}\")\n",
    "            context_parts.append(f\"Source: {results['wikipedia']['url']}\\\\n\")\n",
    "        \n",
    "        if web_results:\n",
    "            context_parts.append(\"Web Search Results:\\\\n\")\n",
    "            for i, result in enumerate(web_results, 1):\n",
    "                context_parts.append(f\"{i}. {result['title']}\")\n",
    "                context_parts.append(f\"   {result['snippet']}\")\n",
    "                context_parts.append(f\"   URL: {result['url']}\\\\n\")\n",
    "        \n",
    "        results['formatted_context'] = '\\\\n'.join(context_parts)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear any cached search results\"\"\"\n",
    "        pass\n",
    "\n",
    "# Initialize Web Search system\n",
    "web_search = WebSearchSystem()\n",
    "\n",
    "print(\"\\\\n=== Web Search System Ready ===\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d9f5f",
   "metadata": {},
   "source": [
    "## Step 5: Create Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def qwen_inference(text_prompt, image_data=None, audio_data=None, video_data=None, \n",
    "                   use_rag=False, use_web_search=False, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Qwen3-Omni multimodal inference\n",
    "    Supports text, image, audio, and video inputs\n",
    "    Now with web search capabilities!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        context_parts = []\n",
    "        \n",
    "        # Prepare context with RAG if enabled\n",
    "        if use_rag and text_prompt:\n",
    "            retrieved_docs = rag_system.retrieve(text_prompt)\n",
    "            if retrieved_docs:\n",
    "                rag_context = \"\\n\\n\".join(retrieved_docs)\n",
    "                context_parts.append(f\"Document Context:\\n{rag_context}\")\n",
    "        \n",
    "        # Add web search results if enabled\n",
    "        if use_web_search and text_prompt:\n",
    "            search_results = web_search.comprehensive_search(\n",
    "                text_prompt, \n",
    "                max_results=5,\n",
    "                include_wikipedia=True\n",
    "            )\n",
    "            if search_results['formatted_context']:\n",
    "                context_parts.append(f\"Web Search Results:\\n{search_results['formatted_context']}\")\n",
    "        \n",
    "        # Combine all context sources\n",
    "        if context_parts:\n",
    "            combined_context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "            text_prompt = f\"{combined_context}\\n\\n---\\n\\nBased on the above information, please answer:\\n{text_prompt}\"\n",
    "        \n",
    "        # Prepare inputs for multimodal processing\n",
    "        inputs = {\"text\": text_prompt}\n",
    "        \n",
    "        # Add image if provided\n",
    "        if image_data:\n",
    "            if isinstance(image_data, str) and image_data.startswith('data:image'):\n",
    "                image_data = base64.b64decode(image_data.split(',')[1])\n",
    "            image = Image.open(BytesIO(image_data)) if isinstance(image_data, bytes) else image_data\n",
    "            inputs[\"image\"] = image\n",
    "        \n",
    "        # Add audio if provided\n",
    "        if audio_data:\n",
    "            inputs[\"audio\"] = audio_data\n",
    "        \n",
    "        # Add video if provided (process first frame or keyframes)\n",
    "        if video_data:\n",
    "            inputs[\"video\"] = video_data\n",
    "        \n",
    "        # Process inputs\n",
    "        model_inputs = qwen_processor(**inputs, return_tensors=\"pt\", padding=True)\n",
    "        model_inputs = {k: v.to(qwen_model.device) for k, v in model_inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = qwen_model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = qwen_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Remove the input prompt from response if it's included\n",
    "        if text_prompt in response:\n",
    "            response = response.split(text_prompt)[-1].strip()\n",
    "        \n",
    "        return {\"response\": response, \"model\": \"Qwen3-Omni-30B\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"model\": \"Qwen3-Omni-30B\"}\n",
    "\n",
    "\n",
    "def gpt_inference(text_prompt, use_rag=False, use_web_search=False, max_new_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    GPT-OSS text-only inference\n",
    "    Now with web search capabilities!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        context_parts = []\n",
    "        \n",
    "        # Prepare context with RAG if enabled\n",
    "        if use_rag:\n",
    "            retrieved_docs = rag_system.retrieve(text_prompt)\n",
    "            if retrieved_docs:\n",
    "                rag_context = \"\\n\\n\".join(retrieved_docs)\n",
    "                context_parts.append(f\"Document Context:\\n{rag_context}\")\n",
    "        \n",
    "        # Add web search results if enabled\n",
    "        if use_web_search:\n",
    "            search_results = web_search.comprehensive_search(\n",
    "                text_prompt, \n",
    "                max_results=5,\n",
    "                include_wikipedia=True\n",
    "            )\n",
    "            if search_results['formatted_context']:\n",
    "                context_parts.append(f\"Web Search Results:\\n{search_results['formatted_context']}\")\n",
    "        \n",
    "        # Combine all context sources\n",
    "        if context_parts:\n",
    "            combined_context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "            text_prompt = f\"{combined_context}\\n\\n---\\n\\nBased on the above information, please answer:\\n{text_prompt}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = gpt_tokenizer(text_prompt, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(gpt_model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=gpt_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove the input prompt from response if it's included\n",
    "        if text_prompt in response:\n",
    "            response = response.split(text_prompt)[-1].strip()\n",
    "        \n",
    "        return {\"response\": response, \"model\": \"GPT-OSS-120B\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"model\": \"GPT-OSS-120B\"}\n",
    "\n",
    "print(\"‚úì Inference functions ready (with web search support!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaf1ff",
   "metadata": {},
   "source": [
    "## Step 6: Setup FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46449242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI(title=\"Multi-Model AI API with RAG & Web Search\")\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request models\n",
    "class TextRequest(BaseModel):\n",
    "    prompt: str\n",
    "    use_rag: bool = False\n",
    "    use_web_search: bool = False\n",
    "    max_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "\n",
    "class MultimodalRequest(BaseModel):\n",
    "    prompt: str\n",
    "    use_rag: bool = False\n",
    "    use_web_search: bool = False\n",
    "    max_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "    image_base64: Optional[str] = None\n",
    "    audio_base64: Optional[str] = None\n",
    "\n",
    "class WebSearchRequest(BaseModel):\n",
    "    query: str\n",
    "    max_results: int = 5\n",
    "    include_wikipedia: bool = True\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"Multi-Model AI API with RAG & Web Search\",\n",
    "        \"models\": [\n",
    "            \"Qwen3-Omni-30B (Multimodal)\",\n",
    "            \"GPT-OSS-120B (Text)\"\n",
    "        ],\n",
    "        \"features\": [\n",
    "            \"RAG (Document Context)\",\n",
    "            \"Web Search (DuckDuckGo + Wikipedia)\",\n",
    "            \"Multimodal Processing\"\n",
    "        ],\n",
    "        \"endpoints\": [\"/qwen\", \"/gpt\", \"/web-search\", \"/upload-docs\", \"/clear-rag\", \"/health\"]\n",
    "    }\n",
    "\n",
    "@app.post(\"/qwen\")\n",
    "async def qwen_endpoint(request: MultimodalRequest):\n",
    "    \"\"\"Qwen3-Omni multimodal inference endpoint with RAG and web search\"\"\"\n",
    "    try:\n",
    "        # Decode base64 data if provided\n",
    "        image_data = None\n",
    "        if request.image_base64:\n",
    "            image_data = base64.b64decode(request.image_base64)\n",
    "        \n",
    "        audio_data = None\n",
    "        if request.audio_base64:\n",
    "            audio_data = base64.b64decode(request.audio_base64)\n",
    "        \n",
    "        result = qwen_inference(\n",
    "            text_prompt=request.prompt,\n",
    "            image_data=image_data,\n",
    "            audio_data=audio_data,\n",
    "            use_rag=request.use_rag,\n",
    "            use_web_search=request.use_web_search,\n",
    "            max_new_tokens=request.max_tokens,\n",
    "            temperature=request.temperature\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/gpt\")\n",
    "async def gpt_endpoint(request: TextRequest):\n",
    "    \"\"\"GPT-OSS text-only inference endpoint with RAG and web search\"\"\"\n",
    "    try:\n",
    "        result = gpt_inference(\n",
    "            text_prompt=request.prompt,\n",
    "            use_rag=request.use_rag,\n",
    "            use_web_search=request.use_web_search,\n",
    "            max_new_tokens=request.max_tokens,\n",
    "            temperature=request.temperature\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/web-search\")\n",
    "async def web_search_endpoint(request: WebSearchRequest):\n",
    "    \"\"\"Standalone web search endpoint\"\"\"\n",
    "    try:\n",
    "        results = web_search.comprehensive_search(\n",
    "            query=request.query,\n",
    "            max_results=request.max_results,\n",
    "            include_wikipedia=request.include_wikipedia\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/upload-docs\")\n",
    "async def upload_documents(files: List[UploadFile] = File(...)):\n",
    "    \"\"\"Upload documents for RAG\"\"\"\n",
    "    try:\n",
    "        saved_files = []\n",
    "        file_types = []\n",
    "        temp_dir = \"/content/uploaded_docs\"\n",
    "        Path(temp_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = Path(temp_dir) / file.filename\n",
    "            content = await file.read()\n",
    "            \n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            saved_files.append(str(file_path))\n",
    "            file_extension = file.filename.split('.')[-1].lower()\n",
    "            file_types.append(file_extension)\n",
    "        \n",
    "        # Add to RAG system\n",
    "        num_chunks = rag_system.add_documents(saved_files, file_types)\n",
    "        \n",
    "        return {\n",
    "            \"message\": f\"Successfully processed {len(files)} documents\",\n",
    "            \"chunks_added\": num_chunks\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/clear-rag\")\n",
    "async def clear_rag():\n",
    "    \"\"\"Clear RAG database\"\"\"\n",
    "    try:\n",
    "        rag_system.clear_database()\n",
    "        return {\"message\": \"RAG database cleared successfully\"}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
    "        \"models_loaded\": True,\n",
    "        \"features\": {\n",
    "            \"rag\": True,\n",
    "            \"web_search\": True,\n",
    "            \"multimodal\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úì FastAPI app configured with web search support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b00140",
   "metadata": {},
   "source": [
    "## Step 7: Start Server with Ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2753337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your ngrok auth token\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN\"  # Get from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ SERVER STARTED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüì° Public URL: {public_url}\")\n",
    "print(f\"\\nüîó API Documentation: {public_url}/docs\")\n",
    "print(\"\\nüí° Use this URL in your frontend application!\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the connection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the server\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b94dd",
   "metadata": {},
   "source": [
    "## Testing the API (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dab42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPT-OSS endpoint\n",
    "import requests\n",
    "\n",
    "url = f\"{public_url}/gpt\"\n",
    "payload = {\n",
    "    \"prompt\": \"What is artificial intelligence?\",\n",
    "    \"use_rag\": False,\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(\"GPT-OSS Response:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qwen endpoint with text\n",
    "url = f\"{public_url}/qwen\"\n",
    "payload = {\n",
    "    \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "    \"use_rag\": False,\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(\"Qwen3-Omni Response:\")\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
